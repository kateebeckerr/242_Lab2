---
title: "Becker_Lab2"
author: "Kate Becker"
date: 1/18/2024
output: pdf_document
---

Today we will be continuing the pumpkin case study from last week. We will be using the data that you cleaned and split last time (pumpkins_train) and will be comparing our results today to those you have already obtained. Open and run your Lab 1.Rmd as a first step so those objects are available in your Environment.

## Lab 1 required for accessing data and varaiables
```{r}
#Load required packages and read in data
library("tidymodels")
library("tidyverse")
library("dplyr")
library("janitor")
library("corrplot")
library("lubridate")
dat <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/pumpkin-data.csv")

#look at df 
glimpse(dat)
#clean column names
pumpkins <- dat %>% clean_names(case = "snake")

#check for names cleaned
pumpkins %>% names()

#select for variety, city_name, package, city_name, package, low_price, high_price, and date
pumpkins <- pumpkins %>% select(variety, city_name, package, low_price, high_price, date)

#look at the first 5 rows of the dataset
pumpkins %>% slice_head(n = 5)

#load in lubridate dataset
library(lubridate)

#  Extract the month and day from the dates and add as new columns
pumpkins <- pumpkins %>%
  mutate(date = mdy(date),  
         day = yday(date),
         month = month(date))
pumpkins %>% 
  select(-day)

#view the first 7 rows of the dataframe
pumpkins %>% slice_head(n = 7)

#create new column "price" by adding low_price and high_price values and divide by 2

pumpkins <- pumpkins %>% 
  mutate(price = (low_price+ high_price)/2)

#visualize predictor variable, date, and price variable, response variable

ggplot(data = pumpkins, aes(x = day, y = price)) +
  geom_point() +
  ggtitle("Pumpkin Sales Throughout the Year")

# Verify the distinct observations in Package column

pumpkins %>% 
  distinct(package)

#look at first 5 rows of dataframe 
pumpkins %>% slice_head(n = 5)

# Verify the distinct observations in Package column

pumpkins %>% distinct(package)

# Retain only pumpkins with "bushel" in the package column
new_pumpkins <- pumpkins %>% 
    filter(str_detect(package, "bushel"))

#check dimensions
dim(new_pumpkins)

#look at first 10 rows of dataset
new_pumpkins %>% 
  slice_head(n = 10)
# Convert the price if the Package contains fractional bushel values
new_pumpkins <- new_pumpkins %>% 
  mutate(price = case_when(
    str_detect(package, "1 1/9") ~ price/(1.1),
    str_detect(package, "1/2") ~ price*2,
    TRUE ~ price))

# View the first few rows of the data
new_pumpkins %>% 
  slice_head(n = 30)

theme_set(theme_light())

# Make a scatter plot of day and price
new_pumpkins %>% 
  ggplot(mapping = aes(x = day, y = price)) +
  geom_point(size = 1.6)

# Find the average price of pumpkins per month then plot a bar chart

pumpkins %>%
  group_by(month) %>% 
  summarise(mean_price = mean(price)) %>% 
  ggplot(aes(x = month, y = mean_price)) +
  geom_col(fill = "midnightblue", alpha = 0.7) +
  ylab("Pumpkin Price")

pumpkins_recipe <- recipe(price ~ ., data = new_pumpkins) %>%  #the dot means adding all x variables together to yield price 
  step_integer(all_predictors(), zero_based = TRUE) #new data creates a specific recipe coverting new data into set of integers


# Print out the recipe
pumpkins_recipe

#prep the recipe
pumpkins_prep <- prep(pumpkins_recipe)

# Bake the recipe to extract a preprocessed new_pumpkins data
baked_pumpkins <- bake(pumpkins_prep, new_data = NULL)

# Print out the baked data set
baked_pumpkins %>% 
  slice_head(n = 10)

#print correlation between package and price variables

cor(baked_pumpkins$package, baked_pumpkins$price)

#Correlation between price and other vars.
cor(baked_pumpkins$city_name, baked_pumpkins$price)
cor(baked_pumpkins$variety, baked_pumpkins$price)
cor(baked_pumpkins$month, baked_pumpkins$price)

# Obtain correlation matrix
corr_mat <- cor(baked_pumpkins %>% 
                  # Drop columns that are not really informative
                  select(-c(low_price, high_price)))

# Make a correlation plot between the variables
corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")


set.seed(123)
# Split the data into training and test sets
pumpkins_split <- baked_pumpkins %>%  #new_pumpkins should be baked_pumpkins 
  initial_split(prop = 0.8)


# Extract training and test data
pumpkins_train <- training(pumpkins_split)
pumpkins_test <- testing(pumpkins_split)


# Create a recipe for preprocessing the data
lm_pumpkins_recipe <- recipe(price ~ package, data = pumpkins_train) %>% 
  step_integer(all_predictors(), zero_based = TRUE)


# Create a linear model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Hold modeling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(lm_pumpkins_recipe) %>% 
  add_model(lm_spec)

# Print out the workflow
lm_wf

# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = pumpkins_train)

# Print the model coefficients learned 
lm_wf_fit

# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = pumpkins_test)

# Bind predictions to the test set
lm_results <- pumpkins_test %>% 
  select(c(package, price)) %>% 
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>% 
  slice_head(n = 10)

# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = price,
        estimate = .pred)

# Encode package column
package_encode <- lm_pumpkins_recipe %>% 
  prep() %>% 
  bake(new_data = pumpkins_test) %>% 
  select(package)


# Bind encoded package column to the results
 plot_results <- lm_results %>%
 bind_cols(package_encode %>%
               rename(package_integer = package)) %>%
  relocate(package_integer, .after = package)

# Print new results data frame
plot_results %>%
  slice_head(n = 5)

# Make a scatter plot
plot_results %>%
  ggplot(mapping = aes(x = package_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a line of best fit
   geom_line(aes(y = .pred), color = "orange", linewidth = 1.2) +
   xlab("package")


```


Once you have done that, we'll start today's lab by specifying a recipe for a polynomial model.  First we specify a recipe that identifies our variables and data, converts the package variable to a numerical form, and then adds a polynomial effect with step_poly()

## Lab 2

```{r}
# Specify a recipe
poly_pumpkins_recipe <-
  recipe(price ~ package, data = pumpkins_train) %>% #specfies a recipe that identifies variables and data 
  step_integer(all_predictors(), zero_based = TRUE) %>%  #converts package variable to a numerical form 
  step_poly(all_predictors(), degree = 3) #adds a polynomial effect 
```

How did that work? Later we will learn about model tuning that will let us do things like find the optimal value for degree.  For now, we'd like to have a flexible model, so we'll use a relatively large value.

Polynomial regression is still linear regression, so our model specification looks similar to before.

```{r}
# Create a model specification called poly_spec
poly_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```

Question 1: Now take the recipe and model specification that just created and bundle them into a workflow called poly_df.

```{r}
# Bundle recipe and model spec into a workflow
poly_wf <- workflow() %>% 
  add_recipe(poly_pumpkins_recipe) %>% 
  add_model(poly_spec)

# Print out the workflow
poly_wf
  
#step_integer creates a specification of a recipe step that will convert new data into a set of integeres based on original data values 
#creates specificaiton of a recipe step that will create new columns that are basic expansions of variables using polynomials 
```

Question 2: fit a model to the pumpkins_train data using your workflow and assign it to poly_wf_fit

```{r}
# Train the model
poly_wf_fit <- poly_wf %>% 
  fit(data = pumpkins_train)

# Print the model coefficients learned 
poly_wf_fit
```


```{r}
# Make price predictions on test data
poly_results <- poly_wf_fit %>% predict(new_data = pumpkins_test) %>% 
  bind_cols(pumpkins_test %>% select(c(package, price))) %>% 
  relocate(.pred, .after = last_col()) #.pred precits values based on input data

# Print the results
poly_results %>% 
  slice_head(n = 10)
```

Now let's evaluate how the model performed on the test_set using yardstick::metrics().
```{r}
#yardstick::metrics() estimates one or more common performance estimates depending on the class of truth 
metrics(data = poly_results, 
        truth = price, 
        estimate = .pred)
#smaller rmse, rsq, and mae all prove the model to be better 
#mean absolute error 
#yardstick works to estimate one or more common performance estiamtes depending on the class of truth 
```
Question 3: How do the performance metrics differ between the linear model from last week and the polynomial model we fit today?  Which model performs better on predicting the price of different packages of pumpkins?

Compared to the linear model from last week, the polynomial we fit today minimized rmse and mae and maximized rsq clear indicators that this model best fits our data. When applied to our data, the RMSE here implies that on average, this model mispredicts the expected price of different packages by about 328 dollars compared to 722 dollars as observed after using the linear regression. In terms of r-squared, the polynomial model has a much larger R^2, 0.89 compared to 0.49, showing a stronger proportion of the variance in pumpkin price predicted from package type using the polynomial regression. Finally, in terms of MAE, the mean absolute difference between the actual and predicted values is minimized in the polynomial regression and can therefore better predict the price of different packages of pumpkins. In conclusion, the polynomial model does a better job at predicting the price of different packages of pumpkins as seen by the performance metrics. 

Let's visualize our model results.  First prep the results by binding the encoded package variable to them.
```{r}
# Bind encoded package column to the results
poly_results <- poly_results %>% 
  bind_cols(package_encode %>% 
              rename(package_integer = package)) %>% 
  relocate(package_integer, .after = package)


# Print new results data frame
poly_results %>% 
  slice_head(n = 5)
```

OK, now let's take a look! 

Question 4: Create a scatter plot that takes the poly_results and plots package vs. price.  Then draw a line showing our model's predicted values (.pred). Hint: you'll need separate geoms for the data points and the prediction line.
```{r}
# Make a scatter plot
poly_results %>% 
  ggplot(aes(x = package, y = price)) +
  geom_point(color = "forestgreen") +
  geom_line(aes(y = .pred), color = "red", linewidth = 1.2) +
  ggtitle("Polynomial Regression of Package vs. Price")

```

You can see that a curved line fits your data much better.

Question 5: Now make a smoother line by using geom_smooth instead of geom_line and passing it a polynomial formula like this:
geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE)

```{r}
# Make a smoother scatter plot 

poly_results %>% 
  ggplot(aes(x = package, y = price)) + 
  geom_point() +
  geom_smooth(method=lm, se=FALSE) +
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE) 


# view poly_results df and class of package and price variables
View(poly_results)
class(poly_results$package)
class(poly_results$price)


```

OK, now it's your turn to go through the process one more time.
 
Additional assignment components :
6. Choose a new predictor variable (anything not involving package type) in this dataset.
- City_name


7. Determine its correlation with the outcome variable (price).  (Remember we calculated a correlation matrix last week)
```{r}
#find correlation between city_name predictor variable and price response variable
cor(baked_pumpkins$city_name, baked_pumpkins$price)
```
The correlation between the predictor variable (city_name) and the outcome variable (price) is 0.324 approximately.

8. Create and test a model for your new predictor:
  - Create a recipe
  - Build a model specification (linear or polynomial)
  - Bundle the recipe and model specification into a workflow
  - Create a model by fitting the workflow
  - Evaluate model performance on the test data
  - Create a visualization of model performance
  
## Testing the Linear Model
```{r}
#random number generator
set.seed(123)

# Split the data into training and test sets
pumpkins_city_split <- baked_pumpkins %>%  
  initial_split(prop = 0.8)

# Extract training and test data
pumpkins_train2 <- training(pumpkins_city_split)
pumpkins_test2 <- testing(pumpkins_city_split)

# Create a recipe for preprocessing the data
lm_city_recipe <- recipe(price ~ city_name, data = pumpkins_train2) %>% 
  step_integer(all_predictors(), zero_based = TRUE)


# Create a linear model specification
lm_specify <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Hold modeling components in a workflow
lm_workflow <- workflow() %>% 
  add_recipe(lm_city_recipe) %>% 
  add_model(lm_specify)

# Print out the workflow
lm_workflow

# Train the model
lm_workflow_train <- lm_workflow %>% 
  fit(data = pumpkins_train2)

# Print the model coefficients learned 
lm_workflow_train

# Make predictions for the test set
test_predictions <- lm_workflow_train %>% 
  predict(new_data = pumpkins_test2)

# Bind predictions to the test set
results1 <- pumpkins_test2 %>% 
  select(c(city_name, price)) %>% 
  bind_cols(test_predictions)


# Print the first ten rows of the tibble
results1 %>% 
  slice_head(n = 10)

#yardstick::metrics() estimates one or more common performance estimates depending on the class of truth 
metrics(data = results1,
        truth = price,
        estimate = .pred)
```


```{r}
# Encode package column
city_encode <- lm_city_recipe %>% 
  prep() %>% 
  bake(new_data = pumpkins_test2) %>% 
  select(city_name)


# Bind encoded package column to the results
 plot <- results1 %>%
 bind_cols(city_encode %>%
               rename(city_integer = city_name)) %>%
  relocate(city_integer, .after = city_name)

# Print new results data frame
plot %>%
  slice_head(n = 5)

# Make a scatter plot
plot %>%
  ggplot(mapping = aes(x = city_integer, y = price)) +
   geom_point(size = 2) +
   # Overlay a line of best fit
   geom_line(aes(y = .pred), color = "pink", linewidth = 2) +
   xlab("City Name") +
  ggtitle("Linear Regression for the Relationship Between City Name and Price")
```
## Testing the Polynomial Model 
```{r}
#random number generator
set.seed(123)
# Split the data into training and test sets
pumpkins_city_poly <- baked_pumpkins %>%  #new_pumpkins should be baked_pumpkins 
  initial_split(prop = 0.8)


# Extract training and test data
pumpkins_train2 <- training(pumpkins_city_poly)
pumpkins_test2 <- testing(pumpkins_city_poly)

# Create a recipe for preprocessing the data
polynomial_recipe <-
  recipe(price ~ city_name, data = pumpkins_train2) %>% #specfies a recipe that identifies variables and data 
  step_integer(all_predictors(), zero_based = TRUE) %>%  #converts package variable to a numerical form 
  step_poly(all_predictors(), degree = 4) #adds a polynomial effect


# Create a polynomial specification
poly_spec1 <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Bundle recipe and model spec into a workflow
poly_workflow <- workflow() %>% 
  add_recipe(polynomial_recipe) %>% 
  add_model(poly_spec1)

# Print out the workflow
poly_workflow

# Train the model
poly_workflow_fitted <- poly_workflow %>% 
  fit(data = pumpkins_train2)

# Print the model coefficients learned 
poly_workflow_fitted

# Make price predictions on test data
results2 <- poly_workflow_fitted %>% predict(new_data = pumpkins_test2) %>% 
  bind_cols(pumpkins_test2 %>% select(c(city_name, price))) %>% 
  relocate(.pred, .after = last_col()) #.pred precits values based on input data


# Print the results
results1 %>% 
  slice_head(n = 10)

#yardstick::metrics() estimates one or more common performance estimates depending on the class of truth 
metrics(data = results2, truth = price, estimate = .pred)
#smaller rmse, rsq, and mae all prove the model to be better 
#mean absolute error 
#yardstick works to estimate one or more common performance estiamtes depending on the class of truth
```


```{r}
# Bind encoded package column to the results
output <- results2 %>% 
  bind_cols(city_encode %>% 
              rename(city_integer = city_name)) %>% 
  relocate(city_integer, .after = city_name)


# Printed new results
output %>% 
  slice_head(n = 5)

#Linear Regression Plot 
output %>% 
  ggplot(aes(x = city_name, y = price)) + 
  geom_point() +
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "green", size = 1.2, se = FALSE) +
  xlab("City Name")
```
  
Lab 2 due 1/24 at 11:59 PM
